{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 6: Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse notebook será usado para salvar o melhor modelo que encontramos na etapa de treinamento e apresentar o código de deployment. Faremos o deploy utilizando a ferramenta BentoML, e disponibilizaremos o modelo como RestAPI via AWS Ec2 utilizando uma estratégia de containers com Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grupo\n",
    " - Nilo Bemfica (nbmcd)\n",
    " - Pedro Didier (pdm)\n",
    " - Pedro Tenório (ptl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pedro\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bentoml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo os ajustes na base para ficar igual ao que tivemos na etapa 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_int(date_str):\n",
    "    date = pd.to_datetime(date_str)\n",
    "\n",
    "    total = int(date.strftime('%S'))\n",
    "    total += int(date.strftime('%M')) * 60\n",
    "    total += int(date.strftime('%H')) * 60 * 60\n",
    "    total += (int(date.strftime('%j')) - 1) * 60 * 60 * 24\n",
    "    # 2002 é o ano mais antigo no dataset\n",
    "    total += (int(date.strftime('%Y')) - 2002) * 60 * 60 * 24 * 365\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "# Carregando base\n",
    "df = pd.read_csv(\"homologated_data.csv\")\n",
    "# Corrigindo formatos de dado\n",
    "df = df.drop(columns=[\"C_man\", \"C_api\"])\n",
    "df = df[df[\"gender\"] != 0].reset_index(drop=True)\n",
    "df[\"firstDay\"] = df[\"firstDay\"].apply(time_to_int)\n",
    "df[\"lastDay\"] = df[\"lastDay\"].apply(time_to_int)\n",
    "# Train test split com seed que utilizamos no experimento\n",
    "X = df.drop(columns=[\"gender\"]).values\n",
    "y = df[\"gender\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    random_state=seed_value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando o modelo como Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=7, min_samples_leaf=4, min_samples_split=10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecionando os melhores parametros\n",
    "clf = RandomForestClassifier(n_estimators=100, min_samples_split=10, min_samples_leaf=4, max_depth=7, bootstrap=True)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy com BentoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos carregar o modelo e salvar como um bentoml model, para poder utilizar as funcionalidades do Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_model.pkl', 'rb') as file:\n",
    "    clf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = bentoml.sklearn.save_model(\"rf_gender_classifier\", clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando um endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em bentoml, para criar uma API que sirva o modelo, basta criar um 'service' com um endpoint que utilize um 'runner' do modelo que você salvou. Um runner é nada mais que um tipo de worker específico do bentoml, que pode ser spawnado diversas vezes conforme a demanda da aplicação.  \n",
    "\n",
    "* Para informações mais detalhadas, aqui está a documentação do BentoML: https://docs.bentoml.com/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bentoml.io import NumpyNdarray\n",
    "\n",
    "gender_clf_runner = bentoml.sklearn.get(\"rf_gender_classifier:latest\").to_runner()\n",
    "\n",
    "svc = bentoml.Service(\"gender_classifier\", runners=[gender_clf_runner])\n",
    "\n",
    "@svc.api(input=NumpyNdarray(), output=NumpyNdarray())\n",
    "def classify(input_series: np.ndarray) -> np.ndarray:\n",
    "    result = gender_clf_runner.predict.run(input_series)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, já é possível servir o modelo localmente, podemos rodar no terminal:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bentoml serve service:svc\n",
    "# Rodar no notebook não vai te permirtir rodar mais nenhuma outra célula, então se for testar rode em um terminal com um python env com os requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E fazer uma request para testar o endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Linha a ser inferida\n",
    "to_predict = [list(X_test[0])]\n",
    "\n",
    "# Fazendo um post para nosso endpoint\n",
    "response = requests.post(\n",
    "   \"http://127.0.0.1:3000/classify\",\n",
    "   headers={\"content-type\": \"application/json\"},\n",
    "   data=str(to_predict),\n",
    ").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linha prevista como [1], onde 1 é homem e 2 é mulher.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linha prevista como {response}, onde 1 é homem e 2 é mulher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para criar um docker da aplicação basta definir um arquivo de deployment chamado **bentofile.yaml** da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "\n",
    "service: \"service:svc\"  # Same as the argument passed to `bentoml serve`\n",
    "labels:\n",
    "   owner: bentoml-team\n",
    "   stage: dev\n",
    "include:\n",
    "- \"*.py\"  # A pattern for matching which files to include in the Bento\n",
    "python:\n",
    "   packages:  # Additional pip packages required by the Service\n",
    "   - scikit-learn\n",
    "models: # The model to be used for building the Bento.\n",
    "- rf_gender_classifier:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E então seguir os seguintes passos no terminal:\n",
    " * bentoml build  \n",
    "**retorna**  \n",
    "Locking PyPI package versions.  \n",
    " BENTOML  \n",
    "Successfully built Bento(tag=\"rf_gender_classifier:awln3pbmlcmlonry\").\n",
    "\n",
    "\n",
    " * bentoml containerize **rf_gender_classifier:<tag_do_container>** # Dada pelo bentoml quando rodamos a linha anterior\n",
    "\n",
    " * bentoml list  \n",
    "**retorna**  \n",
    "Tag                               Size       Creation Time  \n",
    "rf_gender_classifier:awln3pbmlcmlonry  78.84 MiB  2023-09-07 16:38:42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com isso, já temos nosso container da aplicação localmente com Docker. Podemos testar isso com:\n",
    " * docker run -p 3000:3000 rf_gender_classifier:awln3pbmlcmlonry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy no AWS Ec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requerimentos para deploy\n",
    "\n",
    " - Terraform - Terraform: https://www.terraform.io/downloads\n",
    " - AWS CLI - instalado e configurado com permissão para EC2 e ECR: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n",
    " - Docker - Instalação: https://docs.docker.com/install\n",
    " - bentoctl instalado (pip install bentoctl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instale o operator necessário  \n",
    "* bentoctl operator install aws-ec2  \n",
    "\n",
    "### Inicialize o deployment com o bentoctl\n",
    "* bentoctl init\n",
    "\n",
    "### Aqui, você deve configurar seu deploy\n",
    " * name: nome-do-endpoint-da-ia  \n",
    " * region: us-east-2  \n",
    " * operator: aws-ec2 \n",
    " * instance_type: t2.micro\n",
    " * ami_id: ami-0cf0e376c672104d6\n",
    " * enable_gpus: False\n",
    " * sem variáveis de ambiente\n",
    " * escolha um nome para o deployment_config\n",
    "\n",
    "Inicie o terraform \n",
    " * terraform init\n",
    "\n",
    "E, por fim, aplique o terraform\n",
    " * terraform apply -var-file=bentoctl.tfvars -auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com todos os passos seguidos corretamente, conseguimos disponibilizar a API em um endpoint público através do AWS Ec2, testando com um requisição post ao IP público da instância. Não vamos disponibilizar as infomações do endpoint pois desligamos para não gastar com a conta AWS. A t2.micro apesar de estar em free-tier tem uma limitação de tempo e não queremos correr o risco. Caso seja de interesse, podemos subir a aplicação novamente para mostrar o endpoint funcionando.  \n",
    "\n",
    "\n",
    "Outro ponto interessante sobre o bentoml é que ele já possui uma integração fácil com o Prometheus através do endpoint /metrics/ que é criado automaticamente. Isso ajuda bastante num cenário onde é preciso monitorar o funcionamento do deploy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
